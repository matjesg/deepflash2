{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp models\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "> Pytorch segmentation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from fastai.layers import PixelShuffle_ICNR, ConvLayer\n",
    "from fastcore.utils import store_attr\n",
    "from torchvision.models.resnet import ResNet, Bottleneck\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch implementation adapted from https://github.com/jvanvugt/pytorch-unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class UNetConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, padding, batch_norm,\n",
    "                 dropout=0., neg_slope=0.1):\n",
    "        super(UNetConvBlock, self).__init__()\n",
    "        block = []\n",
    "\n",
    "        if dropout>0.:\n",
    "            block.append(nn.Dropout(p=dropout))\n",
    "        block.append(nn.Conv2d(in_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "        block.append(nn.LeakyReLU(negative_slope=neg_slope))\n",
    "\n",
    "\n",
    "        block.append(nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "        block.append(nn.LeakyReLU(negative_slope=neg_slope))\n",
    "\n",
    "        self.block = nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class UNetUpBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, up_mode, padding, batch_norm,\n",
    "                 dropout=0., neg_slope=0.1):\n",
    "        super(UNetUpBlock, self).__init__()\n",
    "        up_block = []\n",
    "        if dropout>0.:\n",
    "            up_block.append(nn.Dropout(p=dropout))\n",
    "        if up_mode == 'upconv':\n",
    "            up_block.append(nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2))\n",
    "        elif up_mode == 'upsample':\n",
    "            up_block.append(nn.Upsample(mode='bilinear', scale_factor=2, align_corners=True))\n",
    "            up_block.append(nn.Conv2d(in_size, out_size, kernel_size=1))\n",
    "        if batch_norm:\n",
    "            up_block.append(nn.BatchNorm2d(out_size))\n",
    "        up_block.append(nn.LeakyReLU(negative_slope=neg_slope))\n",
    "\n",
    "        self.up = nn.Sequential(*up_block)\n",
    "        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n",
    "\n",
    "    def center_crop(self, layer, target_size):\n",
    "        _, _, layer_height, layer_width = layer.size()\n",
    "        diff_y = (layer_height - target_size[0]) // 2\n",
    "        diff_x = (layer_width - target_size[1]) // 2\n",
    "        return layer[\n",
    "            :, :, diff_y : (diff_y + target_size[0]), diff_x : (diff_x + target_size[1])\n",
    "        ]\n",
    "\n",
    "    def forward(self, x, bridge):\n",
    "        up = self.up(x)\n",
    "        crop1 = self.center_crop(bridge, up.shape[2:])\n",
    "        out = torch.cat([up, crop1], 1)\n",
    "        out = self.conv_block(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class UNet2D(nn.Module):\n",
    "    \"Pytorch U-Net Implementation\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        n_classes=2,\n",
    "        depth=5,\n",
    "        wf=6,\n",
    "        padding=False,\n",
    "        batch_norm=False,\n",
    "        dropout = 0.,\n",
    "        neg_slope=0.,\n",
    "        up_mode='upconv',\n",
    "        **kwargs\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        assert up_mode in ('upconv', 'upsample')\n",
    "        self.padding = padding\n",
    "        self.depth = depth\n",
    "        prev_channels = in_channels\n",
    "        self.down_path = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            if batch_norm:\n",
    "                bn = True if i>0 else False\n",
    "            else:\n",
    "                bn = False\n",
    "            if dropout>0.:\n",
    "                do = dropout if i>2 else 0.\n",
    "            else:\n",
    "                do = 0.\n",
    "            self.down_path.append(\n",
    "                UNetConvBlock(prev_channels, 2 ** (wf + i), padding,\n",
    "                              batch_norm=bn, dropout=do,neg_slope=neg_slope)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.up_path = nn.ModuleList()\n",
    "        for i in reversed(range(depth - 1)):\n",
    "            if batch_norm:\n",
    "                bn = True if i>0 else False\n",
    "            else:\n",
    "                bn = False\n",
    "            if dropout>0.:\n",
    "                do = dropout if i>2 else 0.\n",
    "            else:\n",
    "                do = 0.\n",
    "            self.up_path.append(\n",
    "                UNetUpBlock(prev_channels, 2 ** (wf + i), up_mode, padding,\n",
    "                            batch_norm=bn, dropout=do, neg_slope=neg_slope)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize layer weights\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        blocks = []\n",
    "        for i, down in enumerate(self.down_path):\n",
    "            x = down(x)\n",
    "            if i != len(self.down_path) - 1:\n",
    "                blocks.append(x)\n",
    "                x = F.max_pool2d(x, 2)\n",
    "\n",
    "        for i, up in enumerate(self.up_path):\n",
    "            x = up(x, blocks[-i - 1])\n",
    "\n",
    "        return self.last(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Args__:\\\n",
    "`in_channels` (int): the number of input channels.\\\n",
    "`n_classes` (int): the number of output channels. \\\n",
    "`depth` (int): depth of the network.\\\n",
    "`wf` (int): number of filters in the first layer is 2^wf\n",
    "`padding` (bool): if True, apply padding such that the input shape is the same as the output. This may introduce artifacts\\\n",
    "`batch_norm` (bool): Use BatchNorm after layers with an activation function\\\n",
    "`up_mode` (str): one of 'upconv' or 'upsample'. 'upconv' will use transposed convolutions for learned upsampling. 'upsample' will use bilinear upsampling.\\\n",
    "`neg_slope`(float): Controls the angle of the negative slope for LeakyReLU. Standard ReLU if set to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_MODEL_BASE_URL = 'https://github.com/matjesg/deepflash2/releases/download/model_library/'\n",
    "def _load_pretrained(model, arch, dataset, progress=True):\n",
    "    \"Loads pretrained model weights\"\n",
    "    url = f'{_MODEL_BASE_URL}{dataset}-{arch}.pth'\n",
    "    try:\n",
    "        state_dict = torch.hub.load_state_dict_from_url(url, map_location='cpu', progress=progress)\n",
    "    except:\n",
    "        print(f\"Error: No weights available for model {arch} trained on {dataset}.\")\n",
    "        print(f\"Continuing without pretrained weights.\")\n",
    "        return\n",
    "    try:\n",
    "        if arch in [\"unet_deepflash2\",  \"unet_falk2019\", \"unet_ronnberger2015\", \"unet_custom\"]:\n",
    "            if model.state_dict()['last.weight'].shape != state_dict['last.weight'].shape:\n",
    "                print(f\"No pretrained weights for {model.state_dict()['last.weight'].shape[0]} classes in final layer.\")\n",
    "                state_dict.pop('last.bias')\n",
    "                state_dict.pop('last.weight')\n",
    "        elif arch=='unext50_deepflash2':\n",
    "            if model.state_dict()['final_conv.0.weight'].shape != state_dict['final_conv.0.weight'].shape:\n",
    "                print(f\"No pretrained weights for {model.state_dict()['final_conv.0.weight'].shape[0]} classes in final layer.\")\n",
    "                state_dict.pop('final_conv.0.bias')\n",
    "                state_dict.pop('final_conv.0.weight')\n",
    "\n",
    "        # TODO Better handle different number of input channels\n",
    "        _ = model.load_state_dict(state_dict, strict=False)\n",
    "        print(f\"Loaded model weights trained on {dataset}.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error: {e}')\n",
    "        print(f\"Continuing without pretrained weights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original U-Net architecture based on _Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def unet_ronneberger2015(in_channels=1 ,n_classes=2, pretrained=None, progress=True, **kwargs):\n",
    "    \"Original U-Net architecture based on Ronnberger et al. (2015)\"\n",
    "    model = UNet2D(in_channels=in_channels, n_classes=n_classes,\n",
    "                   depth=5, wf=6, padding=False, batch_norm=False,\n",
    "                   neg_slope=0., up_mode='upconv', dropout=0, **kwargs)\n",
    "    if pretrained is not None:\n",
    "        _load_pretrained(model, arch='unet_deepflash2', dataset=pretrained, progress=progress)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net architecture based on _Falk, Thorsten, et al. \"U-Net: deep learning for cell counting, detection, and morphometry.\" Nature methods 16.1 (2019): 67-70._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def unet_falk2019(in_channels=1 ,n_classes=2, pretrained=None, progress=True, **kwargs):\n",
    "    \"U-Net architecture based on Falk et al. (2019)\"\n",
    "    model = UNet2D(in_channels=in_channels, n_classes=n_classes,\n",
    "               depth=5, wf=6, padding=False, batch_norm=False,\n",
    "               neg_slope=0.1, up_mode='upconv', dropout=0, **kwargs)\n",
    "    if pretrained is not None:\n",
    "        _load_pretrained(model, arch='unet_deepflash2', dataset=pretrained, progress=progress)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net model optimized for deepflash2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def unet_deepflash2(in_channels=1 ,n_classes=2, pretrained=None, progress=True, dropout=.5, **kwargs):\n",
    "    \"U-Net model optimized for deepflash2\"\n",
    "    model = UNet2D(in_channels=in_channels, n_classes=n_classes, dropout=dropout, \n",
    "                   depth=5, wf=6, padding=False, batch_norm=True,\n",
    "                   neg_slope=0.1, up_mode='upconv', **kwargs)\n",
    "    if pretrained is not None:\n",
    "        _load_pretrained(model, arch='unet_deepflash2', dataset=pretrained, progress=progress)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model weights trained on wue_cFOS.\n",
      "No pretrained weights for 3 classes in final layer.\n",
      "Loaded model weights trained on wue_cFOS.\n"
     ]
    }
   ],
   "source": [
    "tst = unet_deepflash2()\n",
    "tst = unet_deepflash2(pretrained='wue_cFOS')\n",
    "tst = unet_deepflash2(n_classes=3, pretrained='wue_cFOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def unet_custom(in_channels=1 ,n_classes=2, pretrained=None, progress=True, **kwargs):\n",
    "    \"Customizable U-Net model. Customize via kwargs\"\n",
    "    model = UNet2D(in_channels=in_channels, n_classes=n_classes, **kwargs)\n",
    "    if pretrained:\n",
    "        print('No pretrained weights available for custom architecture.')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UneXt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UneXt50 Architecture adapted from Maxim Shugaev on [Kaggle](https://www.kaggle.com/iafoss/hubmap-pytorch-fast-ai-starter)\n",
    "\n",
    "- Semi-supervised Imagenet pretrained ResNeXt50 (https://github.com/facebookresearch/semi-supervised-ImageNet1K-models) model as a backbone\n",
    "- Feature Pyramid Network (FPN): additional skip connection \n",
    "- Atrous Spatial Pyramid Pooling (ASPP) block added between encoder and decoder to increase the receptive field\n",
    "- Decoder upscaling blocks are based on pixel shuffle (https://arxiv.org/pdf/1609.05158.pdf) rather than transposed convolution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FPN(nn.Module):\n",
    "    def __init__(self, input_channels:list, output_channels:list):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Sequential(nn.Conv2d(in_ch, out_ch*2, kernel_size=3, padding=0),\n",
    "             nn.ReLU(inplace=True), nn.BatchNorm2d(out_ch*2),\n",
    "             nn.Conv2d(out_ch*2, out_ch, kernel_size=3, padding=1))\n",
    "            for in_ch, out_ch in zip(input_channels, output_channels)])\n",
    "        \n",
    "    def forward(self, xs:list, last_layer):\n",
    "        #hcs = [F.interpolate(c(x),scale_factor=2**(len(self.convs)-i),mode='bilinear', align_corners=True) \n",
    "        #       for i,(c,x) in enumerate(zip(self.convs, xs))]\n",
    "        pad_list = [-14,-6,-2,0]\n",
    "        hcs = [F.pad(F.interpolate(c(x),scale_factor=2**(len(self.convs)-i),mode='bilinear', align_corners=True),(p, p, p, p))\n",
    "               for i,(c,x,p) in enumerate(zip(self.convs, xs, pad_list))] ## pad last dims by -1 on each side\n",
    "        hcs.append(last_layer)\n",
    "        return torch.cat(hcs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class UnetBlock(nn.Module):\n",
    "    def __init__(self, up_in_c:int, x_in_c:int, nf:int=None, blur:bool=False,\n",
    "                 self_attention:bool=False, padding:int=1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.shuf = PixelShuffle_ICNR(up_in_c, up_in_c//2, blur=blur, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(x_in_c)\n",
    "        ni = up_in_c//2 + x_in_c\n",
    "        nf = nf if nf is not None else max(up_in_c//2,32)\n",
    "        self.conv1 = ConvLayer(ni, nf, norm_type=None, padding=padding, **kwargs)\n",
    "        self.conv2 = ConvLayer(nf, nf, norm_type=None, padding=padding,\n",
    "            xtra=SelfAttention(nf) if self_attention else None, **kwargs)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, up_in:Tensor, left_in:Tensor) -> Tensor:\n",
    "        s = left_in\n",
    "        up_out = self.shuf(up_in)\n",
    "        cat_x = self.relu(torch.cat([up_out, self.bn(s)], dim=1))\n",
    "        return self.conv2(self.conv1(cat_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _ASPPModule(nn.Module):\n",
    "    def __init__(self, inplanes, planes, kernel_size, padding, dilation, groups=1):\n",
    "        super().__init__()\n",
    "        self.atrous_conv = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,\n",
    "                stride=1, padding=padding, dilation=dilation, bias=False, groups=groups)\n",
    "        self.bn = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.atrous_conv(x)\n",
    "        x = self.bn(x)\n",
    "\n",
    "        return self.relu(x)\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, inplanes=512, mid_c=256, dilations=[6, 12, 18, 24], out_c=None):\n",
    "        super().__init__()\n",
    "        self.aspps = [_ASPPModule(inplanes, mid_c, 1, padding=0, dilation=1)] + \\\n",
    "            [_ASPPModule(inplanes, mid_c, 3, padding=d, dilation=d,groups=4) for d in dilations]\n",
    "        self.aspps = nn.ModuleList(self.aspps)\n",
    "        self.global_pool = nn.Sequential(nn.AdaptiveMaxPool2d((1, 1)),\n",
    "                        nn.Conv2d(inplanes, mid_c, 1, stride=1, bias=False),\n",
    "                        nn.BatchNorm2d(mid_c), nn.ReLU())\n",
    "        out_c = out_c if out_c is not None else mid_c\n",
    "        self.out_conv = nn.Sequential(nn.Conv2d(mid_c*(2+len(dilations)), out_c, 1, bias=False),\n",
    "                                    nn.BatchNorm2d(out_c), nn.ReLU(inplace=True))\n",
    "        self.conv1 = nn.Conv2d(mid_c*(2+len(dilations)), out_c, 1, bias=False)\n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.global_pool(x)\n",
    "        xs = [aspp(x) for aspp in self.aspps]\n",
    "        x0 = F.interpolate(x0, size=xs[0].size()[2:], mode='bilinear', align_corners=True)\n",
    "        x = torch.cat([x0] + xs, dim=1)\n",
    "        return self.out_conv(x)\n",
    "    \n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class UneXt50(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        n_classes=2,\n",
    "        stride=1,\n",
    "        inplanes=64,\n",
    "        pre_ssl = True,\n",
    "        **kwargs):\n",
    "        super().__init__()\n",
    "        store_attr('in_channels, n_classes, inplanes, pre_ssl')\n",
    "        #encoder\n",
    "        if pre_ssl: \n",
    "            m = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models','resnext50_32x4d_ssl')\n",
    "        else: \n",
    "            m = ResNet(Bottleneck, [3, 4, 6, 3], groups=32, width_per_group=4)\n",
    "        m.conv1.padding = (0,0)\n",
    "        \n",
    "        if in_channels<3:\n",
    "            #print('Cutting input layer weights to', in_channels, 'channel(s).')\n",
    "            with torch.no_grad():\n",
    "                m.conv1.weight = nn.Parameter(m.conv1.weight[:,:in_channels,...])\n",
    "        elif in_channels>3:\n",
    "            m.conv1 = nn.Conv2d(1, self.inplanes, kernel_size=7, stride=2, bias=False)\n",
    "        \n",
    "        #self.bn1 =  m.bn1 if in_channels==3 else nn.BatchNorm2d(self.inplanes)\n",
    "        self.enc0 = nn.Sequential(m.conv1, m.bn1, nn.ReLU(inplace=True))\n",
    "        self.enc1 = nn.Sequential(nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1),\n",
    "                            m.layer1) #256\n",
    "        self.enc2 = m.layer2 #512\n",
    "        self.enc3 = m.layer3 #1024\n",
    "        self.enc4 = m.layer4 #2048\n",
    "        #aspp with customized dilatations\n",
    "        self.aspp = ASPP(2048,256,out_c=512,dilations=[stride*1,stride*2,stride*3,stride*4])\n",
    "        self.drop_aspp = nn.Dropout2d(0.5)\n",
    "        #decoder\n",
    "        self.dec4 = UnetBlock(512,1024,256, padding=0)\n",
    "        self.dec3 = UnetBlock(256,512,128, padding=0)\n",
    "        self.dec2 = UnetBlock(128,256,64, padding=0)\n",
    "        self.dec1 = UnetBlock(64,64,32, padding=0)\n",
    "        self.fpn = FPN([512,256,128,64],[16]*4)\n",
    "        self.drop = nn.Dropout2d(0.1)\n",
    "        self.final_conv = ConvLayer(32+16*4, n_classes, ks=1, norm_type=None, act_cls=None)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        c = [-4,-12,-28] #crop list\n",
    "        enc0 = self.enc0(x)\n",
    "        enc1 = self.enc1(enc0)\n",
    "        enc2 = self.enc2(enc1)\n",
    "        enc3 = self.enc3(enc2)\n",
    "        enc4 = self.enc4(enc3)\n",
    "        enc5 = self.aspp(enc4)\n",
    "        dec3 = self.dec4(self.drop_aspp(enc5),enc3)\n",
    "        enc2 = F.pad(enc2, (c[0], c[0], c[0], c[0]))\n",
    "        dec2 = self.dec3(dec3,enc2)\n",
    "        enc1 = F.pad(enc1, (c[1],c[1], c[1], c[1]))\n",
    "        dec1 = self.dec2(dec2,enc1)\n",
    "        enc0 = F.pad(enc0, (c[2], c[2], c[2], c[2]))\n",
    "        dec0 = self.dec1(dec1,enc0)\n",
    "        x = self.fpn([enc5, dec3, dec2, dec1], dec0)\n",
    "        x = self.final_conv(self.drop(x))\n",
    "        x = F.interpolate(x,scale_factor=2,mode='bilinear',align_corners=True)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def unext50_deepflash2(in_channels=1 ,n_classes=2, pretrained=None, progress=True, **kwargs):\n",
    "    \"UneXt50 model. Customize via kwargs\"\n",
    "    model = UneXt50(in_channels=in_channels, n_classes=n_classes, **kwargs)\n",
    "    if pretrained is not None:\n",
    "        _load_pretrained(model, arch='unext50_deepflash2', dataset=pretrained, progress=progress)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /media/data/home/mag01ud/.cache/torch/hub/facebookresearch_semi-supervised-ImageNet1K-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model weights trained on wue_cFOS.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /media/data/home/mag01ud/.cache/torch/hub/facebookresearch_semi-supervised-ImageNet1K-models_master\n"
     ]
    }
   ],
   "source": [
    "tst = unext50_deepflash2(pretrained='wue_cFOS')\n",
    "tst = unext50_deepflash2(in_channels=3, n_classes=5)\n",
    "x = torch.randn(2, 3, 518, 518)\n",
    "y = tst(x)\n",
    "test_eq(y.shape, [2, 5, 392, 392])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmenation Models Pytorch Integration\n",
    "\n",
    "From the website: \n",
    "\n",
    "- High level API (just two lines to create a neural network)\n",
    "- 9 models architectures for binary and multi class segmentation (including legendary Unet)\n",
    "- 104 available encoders\n",
    "- All encoders have pre-trained weights for faster and better convergence\n",
    "\n",
    "See https://github.com/qubvel/segmentation_models.pytorch for API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def import_smp():\n",
    "    'Import or install segmentation_models_pytorch'\n",
    "    try:\n",
    "        import segmentation_models_pytorch\n",
    "    except:\n",
    "        print('Installing segmentation_models_pytorch. Please wait.')\n",
    "        install_package(\"segmentation_models_pytorch\")\n",
    "        import segmentation_models_pytorch\n",
    "    return segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def load_smp_model(arch, **kwargs):\n",
    "    'Load segmentation_models_pytorch model'\n",
    "    smp = import_smp()\n",
    "    if arch==\"Unet\": return smp.Unet(**kwargs)\n",
    "    elif arch==\"UnetPlusPlus\": return smp.UnetPlusPlus(**kwargs)\n",
    "    elif arch==\"MAnet\":return smp.MAnet(**kwargs)\n",
    "    elif arch==\"FPN\": return smp.FPN(**kwargs)\n",
    "    elif arch==\"PAN\": return smp.PAN(**kwargs)\n",
    "    elif arch==\"PSPNet\": return smp.PSPNet(**kwargs)\n",
    "    elif arch==\"Linknet\": return smp.Linknet(**kwargs)\n",
    "    elif arch==\"DeepLabV3\": return smp.DeepLabV3(**kwargs)\n",
    "    elif arch==\"DeepLabV3Plus\": return smp.DeepLabV3Plus(**kwargs)\n",
    "    else: raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = load_smp_model(arch='DeepLabV3', in_channels=3, classes=5)\n",
    "x = torch.randn(2, 3, 512, 512)\n",
    "y = tst(x)\n",
    "test_eq(y.shape, [2, 5, 512, 512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape Defaults "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for default input and masks shapes, depending on model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_default_shapes(arch):\n",
    "    if arch in [\"unet_deepflash2\",  \"unet_falk2019\", \"unet_ronnberger2015\", \"unet_custom\"]:\n",
    "        return {'tile_shape' : (540, 540), 'padding' : (184, 184)}\n",
    "    \n",
    "    elif arch in [\"unext50_deepflash2\"]:\n",
    "        return {'tile_shape' : (518, 518), 'padding' : (126, 126)}\n",
    "    \n",
    "    else:\n",
    "        return {'tile_shape' : (512, 512), 'padding' : (0, 0)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_learner.ipynb.\n",
      "Converted 01_models.ipynb.\n",
      "Converted 02_data.ipynb.\n",
      "Converted 02a_transforms.ipynb.\n",
      "Converted 03_metrics.ipynb.\n",
      "Converted 04_callbacks.ipynb.\n",
      "Converted 05_losses.ipynb.\n",
      "Converted 06_utils.ipynb.\n",
      "Converted 07_tta.ipynb.\n",
      "Converted 08_gui.ipynb.\n",
      "Converted 09_gt.ipynb.\n",
      "Converted add_information.ipynb.\n",
      "Converted deepflash2.ipynb.\n",
      "Converted gt_estimation.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted model_library.ipynb.\n",
      "Converted predict.ipynb.\n",
      "Converted train.ipynb.\n",
      "Converted tutorial.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
