{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp losses\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses\n",
    "\n",
    "> Implements popular segmentation loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastcore.test import *\n",
    "from fastai.torch_core import TensorImage, TensorMask\n",
    "from fastai.losses import CrossEntropyLossFlat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.loss import _Loss\n",
    "import fastai\n",
    "from fastai.torch_core import TensorBase\n",
    "import segmentation_models_pytorch as smp\n",
    "from deepflash2.utils import import_package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Losses implemented here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "LOSSES = ['CrossEntropyLoss', 'DiceLoss', 'SoftCrossEntropyLoss', 'CrossEntropyDiceLoss',  'JaccardLoss', 'FocalLoss', 'LovaszLoss', 'TverskyLoss', 'Poly1CrossEntropyLoss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Wrapper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper for handling different tensor types from [fastai](https://docs.fast.ai/torch_core.html#TensorBase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class FastaiLoss(_Loss):\n",
    "    'Wrapper class around loss function for handling different tensor types.'\n",
    "    def __init__(self, loss, axis=1):\n",
    "        super().__init__()\n",
    "        self.loss = loss\n",
    "        self.axis=axis\n",
    "        \n",
    "    #def _contiguous(self, x): return TensorBase(x.contiguous())\n",
    "    def _contiguous(self,x):\n",
    "        return TensorBase(x.contiguous()) if isinstance(x,torch.Tensor) else x\n",
    "    \n",
    "    def forward(self, *input):\n",
    "        #input = map(self._contiguous, input)        \n",
    "        input = [self._contiguous(x) for x in input]\n",
    "        return self.loss(*input) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper for combining different losses, adapted from from [pytorch-toolbelt](https://github.com/BloodAxe/pytorch-toolbelt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "# from https://github.com/BloodAxe/pytorch-toolbelt/blob/develop/pytorch_toolbelt/losses/joint_loss.py\n",
    "class WeightedLoss(_Loss):\n",
    "    '''\n",
    "    Wrapper class around loss function that applies weighted with fixed factor.\n",
    "    This class helps to balance multiple losses if they have different scales\n",
    "    '''\n",
    "    def __init__(self, loss, weight=1.0):\n",
    "        super().__init__()\n",
    "        self.loss = loss\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, *input):\n",
    "        return self.loss(*input) * self.weight\n",
    "\n",
    "class JointLoss(_Loss):\n",
    "    'Wrap two loss functions into one. This class computes a weighted sum of two losses.'\n",
    "\n",
    "    def __init__(self, first: nn.Module, second: nn.Module, first_weight=1.0, second_weight=1.0):\n",
    "        super().__init__()\n",
    "        self.first = WeightedLoss(first, first_weight)\n",
    "        self.second = WeightedLoss(second, second_weight)\n",
    "\n",
    "    def forward(self, *input):\n",
    "        return self.first(*input) + self.second(*input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popular segmentation losses\n",
    "\n",
    "The `get_loss()` function loads popular segmentation losses from [Segmenation Models Pytorch](https://github.com/qubvel/segmentation_models.pytorch) and [kornia](https://kornia.readthedocs.io/en/latest/losses.html#semantic-segmentation): \n",
    "- (Soft) CrossEntropy Loss\n",
    "- Dice Loss\n",
    "- Jaccard Loss\n",
    "- Focal Loss\n",
    "- Lovasz Loss\n",
    "- TverskyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class Poly1CrossEntropyLoss(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_classes: int,\n",
    "                 epsilon: float = 1.0,\n",
    "                 reduction: str = \"mean\"):\n",
    "        \"\"\"\n",
    "        Create instance of Poly1CrossEntropyLoss\n",
    "        :param num_classes:\n",
    "        :param epsilon:\n",
    "        :param reduction: one of none|sum|mean, apply reduction to final loss tensor\n",
    "        \"\"\"\n",
    "        super(Poly1CrossEntropyLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.epsilon = epsilon\n",
    "        self.reduction = reduction\n",
    "        return\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        :param logits: tensor of shape [BNHW]\n",
    "        :param labels: tensor of shape [BHW]\n",
    "        :return: poly cross-entropy loss\n",
    "        \"\"\"\n",
    "        labels_onehot = F.one_hot(labels, num_classes=self.num_classes).to(device=logits.device,\n",
    "                                                                           dtype=logits.dtype)\n",
    "        labels_onehot = torch.moveaxis(labels_onehot, -1, 1)\n",
    "        pt = torch.sum(labels_onehot * F.softmax(logits, dim=1), dim=1)\n",
    "        CE = F.cross_entropy(input=logits, target=labels, reduction='none')\n",
    "        poly1 = CE + self.epsilon * (1 - pt)\n",
    "        if self.reduction == \"mean\":\n",
    "            poly1 = poly1.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            poly1 = poly1.sum()\n",
    "        return poly1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=2\n",
    "output = torch.randn(4, n_classes, 356, 356, requires_grad=True)\n",
    "target = torch.randint(0, n_classes, (4, 356, 356))\n",
    "\n",
    "tst = Poly1CrossEntropyLoss(num_classes=n_classes) \n",
    "loss = tst(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def get_loss(loss_name, mode='multiclass', classes=[1], smooth_factor=0., alpha=0.5, beta=0.5, gamma=2.0, reduction='mean', **kwargs):\n",
    "    'Load losses from based on loss_name'\n",
    "    \n",
    "    assert loss_name in LOSSES, f'Select one of {LOSSES}'\n",
    "       \n",
    "    if loss_name==\"CrossEntropyLoss\": \n",
    "        loss = fastai.losses.CrossEntropyLossFlat(axis=1) \n",
    "        \n",
    "    if loss_name==\"SoftCrossEntropyLoss\":   \n",
    "        loss = smp.losses.SoftCrossEntropyLoss(smooth_factor=smooth_factor, **kwargs)\n",
    "\n",
    "    elif loss_name==\"DiceLoss\": \n",
    "        loss = smp.losses.DiceLoss(mode=mode, classes=classes, **kwargs)\n",
    "\n",
    "    elif loss_name==\"JaccardLoss\": \n",
    "        loss = smp.losses.JaccardLoss(mode=mode, classes=classes, **kwargs)\n",
    "\n",
    "    elif loss_name==\"FocalLoss\": \n",
    "        loss = smp.losses.FocalLoss(mode=mode, alpha=alpha, gamma=gamma, reduction=reduction, **kwargs)\n",
    "\n",
    "    elif loss_name==\"LovaszLoss\": \n",
    "        loss = smp.losses.LovaszLoss(mode=mode, **kwargs)\n",
    "\n",
    "    elif loss_name==\"TverskyLoss\": \n",
    "        kornia = import_package('kornia')\n",
    "        loss = kornia.losses.TverskyLoss(alpha=alpha, beta=beta, **kwargs)\n",
    "\n",
    "    elif loss_name==\"CrossEntropyDiceLoss\":\n",
    "        dc = smp.losses.DiceLoss(mode=mode, classes=classes, **kwargs)\n",
    "        ce = fastai.losses.CrossEntropyLossFlat(axis=1)\n",
    "        loss = JointLoss(ce, dc, 1, 1)\n",
    "        \n",
    "    elif loss_name==\"Poly1CrossEntropyLoss\":\n",
    "        loss = Poly1CrossEntropyLoss(num_classes=max(classes)+1) \n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing CrossEntropyLoss\n",
      "Testing DiceLoss\n",
      "Testing SoftCrossEntropyLoss\n",
      "Testing CrossEntropyDiceLoss\n",
      "Testing JaccardLoss\n",
      "Testing FocalLoss\n",
      "Testing LovaszLoss\n",
      "Testing TverskyLoss\n",
      "Installing kornia. Please wait.\n",
      "Collecting kornia\n",
      "  Downloading kornia-0.6.4-py2.py3-none-any.whl (493 kB)\n",
      "Requirement already satisfied: torch>=1.8.1 in /home/magr/.conda/envs/fastai2/lib/python3.9/site-packages (from kornia) (1.10.2)\n",
      "Requirement already satisfied: packaging in /home/magr/.local/lib/python3.9/site-packages (from kornia) (21.3)\n",
      "Requirement already satisfied: typing_extensions in /home/magr/.conda/envs/fastai2/lib/python3.9/site-packages (from torch>=1.8.1->kornia) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/magr/.local/lib/python3.9/site-packages (from packaging->kornia) (3.0.7)\n",
      "Installing collected packages: kornia\n",
      "Successfully installed kornia-0.6.4\n",
      "Testing Poly1CrossEntropyLoss\n"
     ]
    }
   ],
   "source": [
    "#Test if all losses are running\n",
    "n_classes = 2\n",
    "#output = TensorImage(torch.randn(4, n_classes, 356, 356, requires_grad=True))\n",
    "#target = TensorMask(torch.randint(0, n_classes, (4, 356, 356)))\n",
    "output = torch.randn(4, n_classes, 356, 356, requires_grad=True)\n",
    "target = torch.randint(0, n_classes, (4, 356, 356))\n",
    "for loss_name in LOSSES:\n",
    "    print(f'Testing {loss_name}')\n",
    "    tst = get_loss(loss_name, classes=list(range(1,n_classes))) \n",
    "    loss = tst(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare soft cross entropy loss with smooth_factor=0 to (fastai) cross entropy \n",
    "ce1 = get_loss('SoftCrossEntropyLoss', smooth_factor=0)\n",
    "ce2 = CrossEntropyLossFlat(axis=1)\n",
    "test_close(ce1(output, target), ce2(output, target), eps=1e-04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare soft cross entropy loss with smooth_factor=0 to cross entropy \n",
    "jc = get_loss('JaccardLoss')\n",
    "dc = get_loss('DiceLoss')\n",
    "dc_loss = dc(output, target)\n",
    "dc_to_jc = 2*dc_loss/(dc_loss+1) #it seems to be the other way around?\n",
    "test_close(jc(output, target), dc_to_jc, eps=1e-02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare TverskyLoss with alpha=0.5 and beta=0.5 to dice loss, should be equal\n",
    "tw = get_loss(\"TverskyLoss\", alpha=0.5, beta=0.5)\n",
    "test_close(dc(output, target), tw(output, target), eps=1e-02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temporaty test if classes are working\n",
    "output = torch.randn(4, n_classes, 356, 356)\n",
    "output[:,1,...] = 0.5\n",
    "tst = get_loss(loss_name='DiceLoss', classes=None) \n",
    "tst2 = get_loss(loss_name='DiceLoss', classes=list(range(1,n_classes))) \n",
    "test_ne(tst(output, target), tst2(output, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_learner.ipynb.\n",
      "Converted 01_models.ipynb.\n",
      "Converted 02_data.ipynb.\n",
      "Converted 05_losses.ipynb.\n",
      "Converted 06_utils.ipynb.\n",
      "Converted 07_tta.ipynb.\n",
      "Converted 08_gui.ipynb.\n",
      "Converted 09_gt.ipynb.\n",
      "Converted add_information.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted model_library.ipynb.\n",
      "Converted tutorial.ipynb.\n",
      "Converted tutorial_gt.ipynb.\n",
      "Converted tutorial_pred.ipynb.\n",
      "Converted tutorial_train.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai2",
   "language": "python",
   "name": "fastai2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
